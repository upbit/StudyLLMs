{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1fbb36-f6a1-4ab5-84b9-bbf372c120b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You may refer to me as Vicuna, a language model meticulously developed by the researchers at Large Model Systems Organization (LMSYS).\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# create a chat completion\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model='vicuna-13b-v1.5',\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n",
    ")\n",
    "# print the completion\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e32a65-a071-47a1-ab11-4a59b6f6b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "\n",
    "guidance.llms.OpenAI.cache.clear()\n",
    "llm = guidance.llms.OpenAI('chatglm2-6b', encoding_name='cl100k_base', chat_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46000574-16f0-4405-9183-f8d28e9b7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS\"\n",
    "\n",
    "context = \"\"\"\n",
    "1 INTRODUCTION\n",
    "Although language models have demonstrated remarkable success across a range of NLP tasks, their\n",
    "ability to demonstrate reasoning is often seen as a limitation, which cannot be overcome solely by\n",
    "increasing model scale (Rae et al., 2021; BIG-bench collaboration, 2021, inter alia). In an effort\n",
    "to address this shortcoming, Wei et al. (2022) have proposed chain-of-thought prompting, where\n",
    "a language model is prompted to generate a series of short sentences that mimic the reasoning\n",
    "process a person might employ in solving a task. For example, given the question “If there are 3\n",
    "cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?”, instead\n",
    "of directly responding with “5”, a language model would be prompted to respond with the entire\n",
    "chain-of-thought: “There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 +\n",
    "2 = 5 cars. The answer is 5.”. It has been observed that chain-of-thought prompting significantly\n",
    "improves model performance across a variety of multi-step reasoning tasks (Wei et al., 2022).\n",
    "In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy\n",
    "decoding strategy used in chain-of-thought prompting (Wei et al., 2022), that further improves\n",
    "language models’ reasoning performance by a significant margin. Self-consistency leverages the\n",
    "intuition that complex reasoning tasks typically admit multiple reasoning paths that reach a correct\n",
    "answer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a\n",
    "problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer.\n",
    "Figure 1 illustrates the self-consistency method with an example. We first prompt the language model\n",
    "with chain-of-thought prompting, then instead of greedily decoding the optimal reasoning path, we\n",
    "propose a “sample-and-marginalize” decoding procedure: we first sample from the language model’s\n",
    "decoder to generate a diverse set of reasoning paths; each reasoning path might lead to a different\n",
    "final answer, so we determine the optimal answer by marginalizing out the sampled reasoning paths\n",
    "to find the most consistent answer in the final answer set. Such an approach is analogous to the\n",
    "human experience that if multiple different ways of thinking lead to the same answer, one has greater\n",
    "confidence that the final answer is correct. Compared to other decoding methods, self-consistency\n",
    "avoids the repetitiveness and local-optimality that plague greedy decoding, while mitigating the\n",
    "stochasticity of a single sampled generation.\n",
    "1\n",
    "arXiv:2203.11171v4 [cs.CL] 7 Mar 2023\n",
    "Published as a conference paper at ICLR 2023\n",
    "Language\n",
    "model\n",
    "Q: If there are 3 cars in the parking\n",
    "lot and 2 more cars arrive, how many\n",
    "cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot\n",
    "already. 2 more arrive. Now there are\n",
    "3 + 2 = 5 cars. The answer is 5. …Q: Janet’s ducks lay 16 eggs per day.\n",
    "She eats three for breakfast every\n",
    "morning and bakes muffins for her\n",
    "friends every day with four. She sells\n",
    "the remainder for $2 per egg. How\n",
    "much does she make every day?\n",
    "A:\n",
    "She has 16 - 3 - 4 = 9 eggs\n",
    "left. So she makes $2 * 9 =\n",
    "$18 per day.\n",
    "Sample a diverse set of\n",
    "reasoning paths\n",
    "She eats 3 for breakfast, so\n",
    "she has 16 - 3 = 13 left. Then\n",
    "she bakes muffins, so she\n",
    "has 13 - 4 = 9 eggs left. So\n",
    "she has 9 eggs * $2 = $18.\n",
    "This means she she sells the\n",
    "remainder for $2 * (16 - 4 - 3)\n",
    "= $26 per day.\n",
    "The answer is $18.\n",
    "The answer is $26.\n",
    "The answer is $18.\n",
    "The answer is $18.\n",
    "Marginalize out reasoning paths\n",
    "to aggregate final answers\n",
    "Language\n",
    "model\n",
    "This means she uses 3 + 4 = 7 eggs every day.\n",
    "She sells the remainder for $2 per egg, so in\n",
    "total she sells 7 * $2 = $14 per day.\n",
    "The answer is $14.\n",
    "The answer is $14.\n",
    "Greedy decode\n",
    "Figure 1: The self-consistency method contains three steps: (1) prompt a language model using\n",
    "chain-of-thought (CoT) prompting; (2) replace the “greedy decode” in CoT prompting by sampling\n",
    "from the language model’s decoder to generate a diverse set of reasoning paths; and (3) marginalize\n",
    "out the reasoning paths and aggregate by choosing the most consistent answer in the final answer set.\n",
    "Self-consistency is far simpler than prior approaches that either train an additional verifier (Cobbe\n",
    "et al., 2021) or train a re-ranker given additional human annotations to improve generation quality\n",
    "(Thoppilan et al., 2022). Instead, self-consistency is entirely unsupervised, works off-the-shelf with\n",
    "pre-trained language models, requires no additional human annotation, and avoids any additional\n",
    "training, auxiliary models or fine-tuning. Self-consistency also differs from a typical ensemble\n",
    "approach where multiple models are trained and the outputs from each model are aggregated, it acts\n",
    "more like a “self-ensemble” that works on top of a single language model.\n",
    "We evaluate self-consistency on a wide range of arithmetic and commonsense reasoning tasks over\n",
    "four language models with varying scales: the public UL2-20B (Tay et al., 2022) and GPT-3-175B\n",
    "(Brown et al., 2020), and two densely-activated decoder-only language models: LaMDA-137B\n",
    "(Thoppilan et al., 2022) and PaLM-540B (Chowdhery et al., 2022). On all four language models,\n",
    "self-consistency improves over chain-of-thought prompting by a striking margin across all tasks. In\n",
    "particular, when used with PaLM-540B or GPT-3, self-consistency achieves new state-of-the-art levels\n",
    "of performance across arithmetic reasoning tasks, including GSM8K (Cobbe et al., 2021) (+17.9%\n",
    "absolute accuracy gains), SVAMP (Patel et al., 2021) (+11.0%), AQuA (Ling et al., 2017) (+12.2%),\n",
    "and across commonsense reasoning tasks such as StrategyQA (Geva et al., 2021) (+6.4%) and ARCchallenge (Clark et al., 2018) (+3.9%). In additional experiments, we show self-consistency can\n",
    "robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance\n",
    "compared to standard prompting (Ye & Durrett, 2022). We also show self-consistency significantly\n",
    "outperforms sample-and-rank, beam search, ensemble-based approaches, and is robust to sampling\n",
    "strategies and imperfect prompts.\n",
    "2 SELF-CONSISTENCY OVER DIVERSE REASONING PATHS\n",
    "A salient aspect of humanity is that people think differently. It is natural to suppose that in tasks\n",
    "requiring deliberate thinking, there are likely several ways to attack the problem. We propose that\n",
    "such a process can be simulated in language models via sampling from the language model’s decoder.\n",
    "For instance, as shown in Figure 1, a model can generate several plausible responses to a math\n",
    "question that all arrive at the same correct answer (Outputs 1 and 3). Since language models are not\n",
    "perfect reasoners, the model might also produce an incorrect reasoning path or make a mistake in\n",
    "one of the reasoning steps (e.g., in Output 2), but such solutions are less likely to arrive at the same\n",
    "answer. That is, we hypothesize that correct reasoning processes, even if they are diverse, tend to\n",
    "have greater agreement in their final answer than incorrect processes.\n",
    "We leverage this intuition by proposing the following self-consistency method. First, a language\n",
    "model is prompted with a set of manually written chain-of-thought exemplars (Wei et al., 2022). Next,\n",
    "2\n",
    "Published as a conference paper at ICLR 2023\n",
    "GSM8K MultiArith AQuA SVAMP CSQA ARC-c\n",
    "Greedy decode 56.5 94.7 35.8 79.0 79.0 85.2\n",
    "Weighted avg (unnormalized) 56.3 ± 0.0 90.5 ± 0.0 35.8 ± 0.0 73.0 ± 0.0 74.8 ± 0.0 82.3 ± 0.0\n",
    "Weighted avg (normalized) 22.1 ± 0.0 59.7 ± 0.0 15.7 ± 0.0 40.5 ± 0.0 52.1 ± 0.0 51.7 ± 0.0\n",
    "Weighted sum (unnormalized) 59.9 ± 0.0 92.2 ± 0.0 38.2 ± 0.0 76.2 ± 0.0 76.2 ± 0.0 83.5 ± 0.0\n",
    "Weighted sum (normalized) 74.1 ± 0.0 99.3 ± 0.0 48.0 ± 0.0 86.8 ± 0.0 80.7 ± 0.0 88.7 ± 0.0\n",
    "Unweighted sum (majority vote) 74.4 ± 0.1 99.3 ± 0.0 48.3 ± 0.5 86.6 ± 0.1 80.7 ± 0.1 88.7 ± 0.1\n",
    "Table 1: Accuracy comparison of different answer aggregation strategies on PaLM-540B.\n",
    "we sample a set of candidate outputs from the language model’s decoder, generating a diverse set of\n",
    "candidate reasoning paths. Self-consistency is compatible with most existing sampling algorithms,\n",
    "including temperature sampling (Ackley et al., 1985; Ficler & Goldberg, 2017), top-k sampling (Fan\n",
    "et al., 2018; Holtzman et al., 2018; Radford et al., 2019), and nucleus sampling (Holtzman et al.,\n",
    "2020). Finally, we aggregate the answers by marginalizing out the sampled reasoning paths and\n",
    "choosing the answer that is the most consistent among the generated answers.\n",
    "In more detail, assume the generated answers ai are from a fixed answer set, ai ∈ A, where\n",
    "i = 1, . . . , m indexes the m candidate outputs sampled from the decoder. Given a prompt and a\n",
    "question, self-consistency introduces an additional latent variable ri\n",
    ", which is a sequence of tokens\n",
    "representing the reasoning path in the i-th output, then couples the generation of (ri\n",
    ", ai) where\n",
    "ri → ai\n",
    ", i.e., generating a reasoning path ri\n",
    "is optional and only used to reach the final answer ai\n",
    ". As\n",
    "an example, consider Output 3 from Figure 1: the first few sentences “She eats 3 for breakfast ... So\n",
    "she has 9 eggs * $2 = $18.” constitutes ri\n",
    ", while the answer 18 from the last sentence, “The answer\n",
    "is $18”, is parsed as ai\n",
    ".\n",
    "1 After sampling multiple (ri\n",
    ", ai) from the model’s decoder, self-consistency\n",
    "applies a marginalization over ri by taking a majority vote over ai\n",
    ", i.e., arg maxa\n",
    "Pm\n",
    "i=1 1(ai = a),\n",
    "or as we defined as the most “consistent” answer among the final answer set.\n",
    "In Table 1, we show the test accuracy over a set of reasoning tasks by using different answer\n",
    "aggregation strategies. In addition to majority vote, one can also weight each (ri\n",
    ", ai) by P(ri\n",
    ", ai\n",
    "|\n",
    "prompt, question) when aggregating the answers. Note to compute P(ri\n",
    ", ai\n",
    "| prompt, question), we\n",
    "can either take the unnormalized probability of the model generating (ri\n",
    ", ai) given (prompt, question),\n",
    "or we can normalize the conditional probability by the output length (Brown et al., 2020), i.e.,\n",
    "P(ri\n",
    ", ai\n",
    "| prompt, question) = exp 1\n",
    "K\n",
    "PK\n",
    "k=1 log P (tk|prompt,question,t1,...,tk−1)\n",
    ", (1)\n",
    "where log P(tk | prompt, question, t1, . . . , tk−1) is the log probability of generating the k-th token\n",
    "tk in (ri\n",
    ", ai) conditioned on the previous tokens, and K is the total number of tokens in (ri\n",
    ", ai).\n",
    "In Table 1, we show that taking the “unweighted sum”, i.e., taking a majority vote directly over ai\n",
    "yields a very similar accuracy as aggregating using the “normalized weighted sum”. We took a closer\n",
    "look at the model’s output probabilities and found this is because for each (ri\n",
    ", ai), the normalized\n",
    "conditional probabilities P(ri\n",
    ", ai\n",
    "| prompt, question) are quite close to each other, i.e., the language\n",
    "model regards those generations as “similarly likely”.2 Additionally, when aggregating the answers,\n",
    "the results in Table 1 show that the “normalized” weighted sum (i.e., Equation 1) yields a much\n",
    "higher accuracy compared to its unnormalized counterpart. For completeness, in Table 1 we also\n",
    "report the results by taking a “weighted average”, i.e., each a gets a score of its weighted sum divided\n",
    "by Pm\n",
    "i=1 1(ai = a), which results in a much worse performance.\n",
    "Self-consistency explores an interesting space between open-ended text generation and optimal\n",
    "text generation with a fixed answer. Reasoning tasks typically have fixed answers, which is why\n",
    "researchers have generally considered greedy decoding approaches (Radford et al., 2019; Wei et al.,\n",
    "2022; Chowdhery et al., 2022). However, we have found that even when the desired answer is fixed,\n",
    "introducing diversity in the reasoning processes can be highly beneficial; therefore we leverage\n",
    "1The parser is task dependent. For arithmetic reasoning, we parse the first numerical part as the final answer\n",
    "after the model generates “The answer is ”. For commonsense reasoning, we parse the full string answer as the\n",
    "final answer after the model generates “The answer is ”. Most generated outputs have a consistent format of\n",
    "“{Reasoning paths}. The answer is X.” if we prompt the language model in this format.\n",
    "2This also means that the language model is not well calibrated and thus cannot distinguish well between\n",
    "correct solutions and wrong solutions, which also explains why additional re-rankers were trained to better judge\n",
    "the quality of the solutions in previous work (Cobbe et al., 2021; Thoppilan et al., 2022).\n",
    "3\n",
    "Published as a conference paper at ICLR 2023\n",
    "sampling, as commonly used for open-ended text generation (Radford et al., 2019; Brown et al., 2020;\n",
    "Thoppilan et al., 2022), to achieve this goal. One should note that self-consistency can be applied\n",
    "only to problems where the final answer is from a fixed answer set, but in principle this approach can\n",
    "be extended to open-text generation problems if a good metric of consistency can be defined between\n",
    "multiple generations, e.g., whether two answers agree or contradict each other.\n",
    "\n",
    "4 RELATED WORK\n",
    "Reasoning in language models. Language models are known to struggle in Type 2 tasks, such as\n",
    "arithmetic, logical and commonsense reasoning (Evans, 2010). Previous work has primarily focused\n",
    "on specialized approaches for improving reasoning (Andor et al., 2019; Ran et al., 2019; Geva et al.,\n",
    "2020; Pi˛ekos et al., 2021). Compared to prior work, self-consistency is applicable to a wide range of\n",
    "reasoning tasks without any additional supervision or fine-tuning, while still substantially improving\n",
    "the performance of the chain-of-thought prompting approach proposed in Wei et al. (2022).\n",
    "Sampling and re-ranking in language models. Multiple decoding strategies for language models\n",
    "have been proposed in the literature, e.g., temperature sampling (Ackley et al., 1985; Ficler &\n",
    "Goldberg, 2017), top-k sampling (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019),\n",
    "nucleus sampling (Holtzman et al., 2020), minimum Bayes risk decoding (Eikema & Aziz, 2020; Shi\n",
    "et al., 2022), and typical decoding (Meister et al., 2022). Other work has sought to explicitly promote\n",
    "diversity in the decoding process (Batra et al., 2012; Li et al., 2016; Vijayakumar et al., 2018).\n",
    "Re-ranking is another common approach to improve generation quality in language models (Adiwardana et al., 2020; Shen et al., 2021). Thoppilan et al. (2022) collect additional human annotations\n",
    "to train a re-ranker for response filtering. Cobbe et al. (2021) train a “verifier” to re-rank generated\n",
    "solutions, which substantially improves the solve rate on math tasks compared to just fine-tuning the\n",
    "language model. Elazar et al. (2021) improve the consistency of factual knowledge extraction by\n",
    "extending pre-training with an additional consistency loss. All these methods require either training\n",
    "an additional re-ranker or collecting additional human annotation, while self-consistency requires no\n",
    "additional training, fine-tuning, nor extra data collection.\n",
    "Extract reasoning paths. Some previous work has considered task-specific approaches for identifying reasoning paths, such as constructing semantic graphs (Xu et al., 2021a), learning an RNN\n",
    "to retrieve reasoning paths over the Wikipedia graph (Asai et al., 2020), fine-tuning with human\n",
    "annotated reasoning paths on math problems (Cobbe et al., 2021), or training an extractor with\n",
    "heuristic-based pseudo reasoning paths (Chen et al., 2019). More recently, the importance of diversity in the reasoning processes has been noticed, but only leveraged via task-specific training,\n",
    "either through an additional QA model over extracted reasoning paths (Chen et al., 2019), or by the\n",
    "introduction of latent variables in a commonsense knowledge graph (Yu et al., 2022). Compared to\n",
    "these approaches, self-consistency is far simpler and requires no additional training. The approach\n",
    "we propose simply couples the generation of reasoning paths and a final answer by sampling from\n",
    "the decoder, using aggregation to recover the most consistent answer without additional modules.\n",
    "Consistency in language models. Some prior work has shown that language models can suffer\n",
    "from inconsistency in conversation (Adiwardana et al., 2020), explanation generation (Camburu et al.,\n",
    "2020), and factual knowledge extraction (Elazar et al., 2021). Welleck et al. (2020) use “consistency”\n",
    "to refer to generating an infinite-length sequence in recurrent language models. Nye et al. (2021)\n",
    "improve the logical consistency of samples from a System 1 model by adding a System 2-inspired\n",
    "logical reasoning module. In this paper we focus on a slightly different notion of “consistency”, i.e.,\n",
    "utilizing answer consistency among diverse reasoning paths to improve accuracy.\n",
    "5 CONCLUSION AND DISCUSSION\n",
    "We introduced a simple yet effective method called self-consistency, and observed that it significantly\n",
    "improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large\n",
    "language models with varying scales. Beyond accuracy gains, self-consistency is also useful for\n",
    "collecting rationales when performing reasoning tasks with language models, and for providing\n",
    "uncertainty estimates and improved calibration of language model outputs.\n",
    "One limitation of self-consistency is that it incurs more computation cost. In practice people can try a\n",
    "small number of paths (e.g., 5 or 10) as a starting point to realize most of the gains while not incurring\n",
    "too much cost, as in most cases the performance saturates quickly (Figure 2). As part of future work,\n",
    "one could use self-consistency to generate better supervised data to fine-tune the model, such that the\n",
    "model can give more accurate predictions in a single inference run after fine-tuning. In addition, we\n",
    "observed that language models can sometimes generate incorrect or nonsensical reasoning paths (e.g.,\n",
    "the StrategyQA example in Table 4, the two population numbers are not exactly correct), and further\n",
    "work is needed to better ground models’ rationale generations.\n",
    "9\n",
    "Published as a conference paper at ICLR 2023\n",
    "REPRODUCIBILITY STATEMENT\n",
    "In experiments, we included four different language models with varying scales. Two of them are public models: UL2 is a completely open-sourced model with model checkpoints available at https://\n",
    "github.com/google-research/google-research/tree/master/ul2; GPT-3 is\n",
    "also a public model with public API available at https://openai.com/api/. For GPT-3,\n",
    "we have included two public engines (“code-davinci-001” and “code-davinci-002”) to further aid\n",
    "reproducibility, as Codex is currently free so anyone can reproduce the results. In addition, as our\n",
    "results make use of LaMDA-137B and PaLM-540B that are not publicly available, we provide the\n",
    "exact input prompts for all tasks in Appendix A.3 (and note that we do not perform any finetuning\n",
    "and only apply prompting to off-the-shelf language models).\n",
    "ETHICS STATEMENT\n",
    "As we stated in the discussion, language models can sometimes generate nonsensical or non-factual\n",
    "reasoning paths, so one should use language models’ outputs with extra caution. We deal with\n",
    "reasoning tasks mostly and the generated rationales are only used for inspecting how a model reaches\n",
    "its answer. One could potentially use the generated rationales to further check why the model makes\n",
    "certain mistakes or whether the model contains any biases when performing a certain task. For\n",
    "language model in real-world use, further work is needed to better ground models’ predictions and\n",
    "improve model’s factuality and safety, to ensure the models do not cause harms to users.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1d7a4f-5605-4c28-83a1-cb10ca05962c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-c6ee5951-9ab4-4a29-b394-f48b8220d498\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-c6ee5951-9ab4-4a29-b394-f48b8220d498\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>你是个拥有丰富经验的教授，负责给学生（用户）解答学术中的问题\n",
       "注意，先提取每段话的关键信息并输出出来，每个关键信息至少列举原文中的3个论据以支撑该观点。注意不要遗漏其中的任何观点</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>教授，请帮助我总结这个段落，列举其中的主要贡献\n",
       "\n",
       "标题：<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{title}}'>SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS</span>\n",
       "正文：<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{context}}'>\n",
       "1 INTRODUCTION\n",
       "Although language models have demonstrated remarkable success across a range of NLP tasks, their\n",
       "ability to demonstrate reasoning is often seen as a limitation, which cannot be overcome solely by\n",
       "increasing model scale (Rae et al., 2021; BIG-bench collaboration, 2021, inter alia). In an effort\n",
       "to address this shortcoming, Wei et al. (2022) have proposed chain-of-thought prompting, where\n",
       "a language model is prompted to generate a series of short sentences that mimic the reasoning\n",
       "process a person might employ in solving a task. For example, given the question “If there are 3\n",
       "cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?”, instead\n",
       "of directly responding with “5”, a language model would be prompted to respond with the entire\n",
       "chain-of-thought: “There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 +\n",
       "2 = 5 cars. The answer is 5.”. It has been observed that chain-of-thought prompting significantly\n",
       "improves model performance across a variety of multi-step reasoning tasks (Wei et al., 2022).\n",
       "In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy\n",
       "decoding strategy used in chain-of-thought prompting (Wei et al., 2022), that further improves\n",
       "language models’ reasoning performance by a significant margin. Self-consistency leverages the\n",
       "intuition that complex reasoning tasks typically admit multiple reasoning paths that reach a correct\n",
       "answer (Stanovich &amp; West, 2000). The more that deliberate thinking and analysis is required for a\n",
       "problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer.\n",
       "Figure 1 illustrates the self-consistency method with an example. We first prompt the language model\n",
       "with chain-of-thought prompting, then instead of greedily decoding the optimal reasoning path, we\n",
       "propose a “sample-and-marginalize” decoding procedure: we first sample from the language model’s\n",
       "decoder to generate a diverse set of reasoning paths; each reasoning path might lead to a different\n",
       "final answer, so we determine the optimal answer by marginalizing out the sampled reasoning paths\n",
       "to find the most consistent answer in the final answer set. Such an approach is analogous to the\n",
       "human experience that if multiple different ways of thinking lead to the same answer, one has greater\n",
       "confidence that the final answer is correct. Compared to other decoding methods, self-consistency\n",
       "avoids the repetitiveness and local-optimality that plague greedy decoding, while mitigating the\n",
       "stochasticity of a single sampled generation.\n",
       "1\n",
       "arXiv:2203.11171v4 [cs.CL] 7 Mar 2023\n",
       "Published as a conference paper at ICLR 2023\n",
       "Language\n",
       "model\n",
       "Q: If there are 3 cars in the parking\n",
       "lot and 2 more cars arrive, how many\n",
       "cars are in the parking lot?\n",
       "A: There are 3 cars in the parking lot\n",
       "already. 2 more arrive. Now there are\n",
       "3 + 2 = 5 cars. The answer is 5. …Q: Janet’s ducks lay 16 eggs per day.\n",
       "She eats three for breakfast every\n",
       "morning and bakes muffins for her\n",
       "friends every day with four. She sells\n",
       "the remainder for $2 per egg. How\n",
       "much does she make every day?\n",
       "A:\n",
       "She has 16 - 3 - 4 = 9 eggs\n",
       "left. So she makes $2 * 9 =\n",
       "$18 per day.\n",
       "Sample a diverse set of\n",
       "reasoning paths\n",
       "She eats 3 for breakfast, so\n",
       "she has 16 - 3 = 13 left. Then\n",
       "she bakes muffins, so she\n",
       "has 13 - 4 = 9 eggs left. So\n",
       "she has 9 eggs * $2 = $18.\n",
       "This means she she sells the\n",
       "remainder for $2 * (16 - 4 - 3)\n",
       "= $26 per day.\n",
       "The answer is $18.\n",
       "The answer is $26.\n",
       "The answer is $18.\n",
       "The answer is $18.\n",
       "Marginalize out reasoning paths\n",
       "to aggregate final answers\n",
       "Language\n",
       "model\n",
       "This means she uses 3 + 4 = 7 eggs every day.\n",
       "She sells the remainder for $2 per egg, so in\n",
       "total she sells 7 * $2 = $14 per day.\n",
       "The answer is $14.\n",
       "The answer is $14.\n",
       "Greedy decode\n",
       "Figure 1: The self-consistency method contains three steps: (1) prompt a language model using\n",
       "chain-of-thought (CoT) prompting; (2) replace the “greedy decode” in CoT prompting by sampling\n",
       "from the language model’s decoder to generate a diverse set of reasoning paths; and (3) marginalize\n",
       "out the reasoning paths and aggregate by choosing the most consistent answer in the final answer set.\n",
       "Self-consistency is far simpler than prior approaches that either train an additional verifier (Cobbe\n",
       "et al., 2021) or train a re-ranker given additional human annotations to improve generation quality\n",
       "(Thoppilan et al., 2022). Instead, self-consistency is entirely unsupervised, works off-the-shelf with\n",
       "pre-trained language models, requires no additional human annotation, and avoids any additional\n",
       "training, auxiliary models or fine-tuning. Self-consistency also differs from a typical ensemble\n",
       "approach where multiple models are trained and the outputs from each model are aggregated, it acts\n",
       "more like a “self-ensemble” that works on top of a single language model.\n",
       "We evaluate self-consistency on a wide range of arithmetic and commonsense reasoning tasks over\n",
       "four language models with varying scales: the public UL2-20B (Tay et al., 2022) and GPT-3-175B\n",
       "(Brown et al., 2020), and two densely-activated decoder-only language models: LaMDA-137B\n",
       "(Thoppilan et al., 2022) and PaLM-540B (Chowdhery et al., 2022). On all four language models,\n",
       "self-consistency improves over chain-of-thought prompting by a striking margin across all tasks. In\n",
       "particular, when used with PaLM-540B or GPT-3, self-consistency achieves new state-of-the-art levels\n",
       "of performance across arithmetic reasoning tasks, including GSM8K (Cobbe et al., 2021) (+17.9%\n",
       "absolute accuracy gains), SVAMP (Patel et al., 2021) (+11.0%), AQuA (Ling et al., 2017) (+12.2%),\n",
       "and across commonsense reasoning tasks such as StrategyQA (Geva et al., 2021) (+6.4%) and ARCchallenge (Clark et al., 2018) (+3.9%). In additional experiments, we show self-consistency can\n",
       "robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance\n",
       "compared to standard prompting (Ye &amp; Durrett, 2022). We also show self-consistency significantly\n",
       "outperforms sample-and-rank, beam search, ensemble-based approaches, and is robust to sampling\n",
       "strategies and imperfect prompts.\n",
       "2 SELF-CONSISTENCY OVER DIVERSE REASONING PATHS\n",
       "A salient aspect of humanity is that people think differently. It is natural to suppose that in tasks\n",
       "requiring deliberate thinking, there are likely several ways to attack the problem. We propose that\n",
       "such a process can be simulated in language models via sampling from the language model’s decoder.\n",
       "For instance, as shown in Figure 1, a model can generate several plausible responses to a math\n",
       "question that all arrive at the same correct answer (Outputs 1 and 3). Since language models are not\n",
       "perfect reasoners, the model might also produce an incorrect reasoning path or make a mistake in\n",
       "one of the reasoning steps (e.g., in Output 2), but such solutions are less likely to arrive at the same\n",
       "answer. That is, we hypothesize that correct reasoning processes, even if they are diverse, tend to\n",
       "have greater agreement in their final answer than incorrect processes.\n",
       "We leverage this intuition by proposing the following self-consistency method. First, a language\n",
       "model is prompted with a set of manually written chain-of-thought exemplars (Wei et al., 2022). Next,\n",
       "2\n",
       "Published as a conference paper at ICLR 2023\n",
       "GSM8K MultiArith AQuA SVAMP CSQA ARC-c\n",
       "Greedy decode 56.5 94.7 35.8 79.0 79.0 85.2\n",
       "Weighted avg (unnormalized) 56.3 ± 0.0 90.5 ± 0.0 35.8 ± 0.0 73.0 ± 0.0 74.8 ± 0.0 82.3 ± 0.0\n",
       "Weighted avg (normalized) 22.1 ± 0.0 59.7 ± 0.0 15.7 ± 0.0 40.5 ± 0.0 52.1 ± 0.0 51.7 ± 0.0\n",
       "Weighted sum (unnormalized) 59.9 ± 0.0 92.2 ± 0.0 38.2 ± 0.0 76.2 ± 0.0 76.2 ± 0.0 83.5 ± 0.0\n",
       "Weighted sum (normalized) 74.1 ± 0.0 99.3 ± 0.0 48.0 ± 0.0 86.8 ± 0.0 80.7 ± 0.0 88.7 ± 0.0\n",
       "Unweighted sum (majority vote) 74.4 ± 0.1 99.3 ± 0.0 48.3 ± 0.5 86.6 ± 0.1 80.7 ± 0.1 88.7 ± 0.1\n",
       "Table 1: Accuracy comparison of different answer aggregation strategies on PaLM-540B.\n",
       "we sample a set of candidate outputs from the language model’s decoder, generating a diverse set of\n",
       "candidate reasoning paths. Self-consistency is compatible with most existing sampling algorithms,\n",
       "including temperature sampling (Ackley et al., 1985; Ficler &amp; Goldberg, 2017), top-k sampling (Fan\n",
       "et al., 2018; Holtzman et al., 2018; Radford et al., 2019), and nucleus sampling (Holtzman et al.,\n",
       "2020). Finally, we aggregate the answers by marginalizing out the sampled reasoning paths and\n",
       "choosing the answer that is the most consistent among the generated answers.\n",
       "In more detail, assume the generated answers ai are from a fixed answer set, ai ∈ A, where\n",
       "i = 1, . . . , m indexes the m candidate outputs sampled from the decoder. Given a prompt and a\n",
       "question, self-consistency introduces an additional latent variable ri\n",
       ", which is a sequence of tokens\n",
       "representing the reasoning path in the i-th output, then couples the generation of (ri\n",
       ", ai) where\n",
       "ri → ai\n",
       ", i.e., generating a reasoning path ri\n",
       "is optional and only used to reach the final answer ai\n",
       ". As\n",
       "an example, consider Output 3 from Figure 1: the first few sentences “She eats 3 for breakfast ... So\n",
       "she has 9 eggs * $2 = $18.” constitutes ri\n",
       ", while the answer 18 from the last sentence, “The answer\n",
       "is $18”, is parsed as ai\n",
       ".\n",
       "1 After sampling multiple (ri\n",
       ", ai) from the model’s decoder, self-consistency\n",
       "applies a marginalization over ri by taking a majority vote over ai\n",
       ", i.e., arg maxa\n",
       "Pm\n",
       "i=1 1(ai = a),\n",
       "or as we defined as the most “consistent” answer among the final answer set.\n",
       "In Table 1, we show the test accuracy over a set of reasoning tasks by using different answer\n",
       "aggregation strategies. In addition to majority vote, one can also weight each (ri\n",
       ", ai) by P(ri\n",
       ", ai\n",
       "|\n",
       "prompt, question) when aggregating the answers. Note to compute P(ri\n",
       ", ai\n",
       "| prompt, question), we\n",
       "can either take the unnormalized probability of the model generating (ri\n",
       ", ai) given (prompt, question),\n",
       "or we can normalize the conditional probability by the output length (Brown et al., 2020), i.e.,\n",
       "P(ri\n",
       ", ai\n",
       "| prompt, question) = exp 1\n",
       "K\n",
       "PK\n",
       "k=1 log P (tk|prompt,question,t1,...,tk−1)\n",
       ", (1)\n",
       "where log P(tk | prompt, question, t1, . . . , tk−1) is the log probability of generating the k-th token\n",
       "tk in (ri\n",
       ", ai) conditioned on the previous tokens, and K is the total number of tokens in (ri\n",
       ", ai).\n",
       "In Table 1, we show that taking the “unweighted sum”, i.e., taking a majority vote directly over ai\n",
       "yields a very similar accuracy as aggregating using the “normalized weighted sum”. We took a closer\n",
       "look at the model’s output probabilities and found this is because for each (ri\n",
       ", ai), the normalized\n",
       "conditional probabilities P(ri\n",
       ", ai\n",
       "| prompt, question) are quite close to each other, i.e., the language\n",
       "model regards those generations as “similarly likely”.2 Additionally, when aggregating the answers,\n",
       "the results in Table 1 show that the “normalized” weighted sum (i.e., Equation 1) yields a much\n",
       "higher accuracy compared to its unnormalized counterpart. For completeness, in Table 1 we also\n",
       "report the results by taking a “weighted average”, i.e., each a gets a score of its weighted sum divided\n",
       "by Pm\n",
       "i=1 1(ai = a), which results in a much worse performance.\n",
       "Self-consistency explores an interesting space between open-ended text generation and optimal\n",
       "text generation with a fixed answer. Reasoning tasks typically have fixed answers, which is why\n",
       "researchers have generally considered greedy decoding approaches (Radford et al., 2019; Wei et al.,\n",
       "2022; Chowdhery et al., 2022). However, we have found that even when the desired answer is fixed,\n",
       "introducing diversity in the reasoning processes can be highly beneficial; therefore we leverage\n",
       "1The parser is task dependent. For arithmetic reasoning, we parse the first numerical part as the final answer\n",
       "after the model generates “The answer is ”. For commonsense reasoning, we parse the full string answer as the\n",
       "final answer after the model generates “The answer is ”. Most generated outputs have a consistent format of\n",
       "“{Reasoning paths}. The answer is X.” if we prompt the language model in this format.\n",
       "2This also means that the language model is not well calibrated and thus cannot distinguish well between\n",
       "correct solutions and wrong solutions, which also explains why additional re-rankers were trained to better judge\n",
       "the quality of the solutions in previous work (Cobbe et al., 2021; Thoppilan et al., 2022).\n",
       "3\n",
       "Published as a conference paper at ICLR 2023\n",
       "sampling, as commonly used for open-ended text generation (Radford et al., 2019; Brown et al., 2020;\n",
       "Thoppilan et al., 2022), to achieve this goal. One should note that self-consistency can be applied\n",
       "only to problems where the final answer is from a fixed answer set, but in principle this approach can\n",
       "be extended to open-text generation problems if a good metric of consistency can be defined between\n",
       "multiple generations, e.g., whether two answers agree or contradict each other.\n",
       "\n",
       "4 RELATED WORK\n",
       "Reasoning in language models. Language models are known to struggle in Type 2 tasks, such as\n",
       "arithmetic, logical and commonsense reasoning (Evans, 2010). Previous work has primarily focused\n",
       "on specialized approaches for improving reasoning (Andor et al., 2019; Ran et al., 2019; Geva et al.,\n",
       "2020; Pi˛ekos et al., 2021). Compared to prior work, self-consistency is applicable to a wide range of\n",
       "reasoning tasks without any additional supervision or fine-tuning, while still substantially improving\n",
       "the performance of the chain-of-thought prompting approach proposed in Wei et al. (2022).\n",
       "Sampling and re-ranking in language models. Multiple decoding strategies for language models\n",
       "have been proposed in the literature, e.g., temperature sampling (Ackley et al., 1985; Ficler &amp;\n",
       "Goldberg, 2017), top-k sampling (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019),\n",
       "nucleus sampling (Holtzman et al., 2020), minimum Bayes risk decoding (Eikema &amp; Aziz, 2020; Shi\n",
       "et al., 2022), and typical decoding (Meister et al., 2022). Other work has sought to explicitly promote\n",
       "diversity in the decoding process (Batra et al., 2012; Li et al., 2016; Vijayakumar et al., 2018).\n",
       "Re-ranking is another common approach to improve generation quality in language models (Adiwardana et al., 2020; Shen et al., 2021). Thoppilan et al. (2022) collect additional human annotations\n",
       "to train a re-ranker for response filtering. Cobbe et al. (2021) train a “verifier” to re-rank generated\n",
       "solutions, which substantially improves the solve rate on math tasks compared to just fine-tuning the\n",
       "language model. Elazar et al. (2021) improve the consistency of factual knowledge extraction by\n",
       "extending pre-training with an additional consistency loss. All these methods require either training\n",
       "an additional re-ranker or collecting additional human annotation, while self-consistency requires no\n",
       "additional training, fine-tuning, nor extra data collection.\n",
       "Extract reasoning paths. Some previous work has considered task-specific approaches for identifying reasoning paths, such as constructing semantic graphs (Xu et al., 2021a), learning an RNN\n",
       "to retrieve reasoning paths over the Wikipedia graph (Asai et al., 2020), fine-tuning with human\n",
       "annotated reasoning paths on math problems (Cobbe et al., 2021), or training an extractor with\n",
       "heuristic-based pseudo reasoning paths (Chen et al., 2019). More recently, the importance of diversity in the reasoning processes has been noticed, but only leveraged via task-specific training,\n",
       "either through an additional QA model over extracted reasoning paths (Chen et al., 2019), or by the\n",
       "introduction of latent variables in a commonsense knowledge graph (Yu et al., 2022). Compared to\n",
       "these approaches, self-consistency is far simpler and requires no additional training. The approach\n",
       "we propose simply couples the generation of reasoning paths and a final answer by sampling from\n",
       "the decoder, using aggregation to recover the most consistent answer without additional modules.\n",
       "Consistency in language models. Some prior work has shown that language models can suffer\n",
       "from inconsistency in conversation (Adiwardana et al., 2020), explanation generation (Camburu et al.,\n",
       "2020), and factual knowledge extraction (Elazar et al., 2021). Welleck et al. (2020) use “consistency”\n",
       "to refer to generating an infinite-length sequence in recurrent language models. Nye et al. (2021)\n",
       "improve the logical consistency of samples from a System 1 model by adding a System 2-inspired\n",
       "logical reasoning module. In this paper we focus on a slightly different notion of “consistency”, i.e.,\n",
       "utilizing answer consistency among diverse reasoning paths to improve accuracy.\n",
       "5 CONCLUSION AND DISCUSSION\n",
       "We introduced a simple yet effective method called self-consistency, and observed that it significantly\n",
       "improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large\n",
       "language models with varying scales. Beyond accuracy gains, self-consistency is also useful for\n",
       "collecting rationales when performing reasoning tasks with language models, and for providing\n",
       "uncertainty estimates and improved calibration of language model outputs.\n",
       "One limitation of self-consistency is that it incurs more computation cost. In practice people can try a\n",
       "small number of paths (e.g., 5 or 10) as a starting point to realize most of the gains while not incurring\n",
       "too much cost, as in most cases the performance saturates quickly (Figure 2). As part of future work,\n",
       "one could use self-consistency to generate better supervised data to fine-tune the model, such that the\n",
       "model can give more accurate predictions in a single inference run after fine-tuning. In addition, we\n",
       "observed that language models can sometimes generate incorrect or nonsensical reasoning paths (e.g.,\n",
       "the StrategyQA example in Table 4, the two population numbers are not exactly correct), and further\n",
       "work is needed to better ground models’ rationale generations.\n",
       "9\n",
       "Published as a conference paper at ICLR 2023\n",
       "REPRODUCIBILITY STATEMENT\n",
       "In experiments, we included four different language models with varying scales. Two of them are public models: UL2 is a completely open-sourced model with model checkpoints available at https://\n",
       "github.com/google-research/google-research/tree/master/ul2; GPT-3 is\n",
       "also a public model with public API available at https://openai.com/api/. For GPT-3,\n",
       "we have included two public engines (“code-davinci-001” and “code-davinci-002”) to further aid\n",
       "reproducibility, as Codex is currently free so anyone can reproduce the results. In addition, as our\n",
       "results make use of LaMDA-137B and PaLM-540B that are not publicly available, we provide the\n",
       "exact input prompts for all tasks in Appendix A.3 (and note that we do not perform any finetuning\n",
       "and only apply prompting to off-the-shelf language models).\n",
       "ETHICS STATEMENT\n",
       "As we stated in the discussion, language models can sometimes generate nonsensical or non-factual\n",
       "reasoning paths, so one should use language models’ outputs with extra caution. We deal with\n",
       "reasoning tasks mostly and the generated rationales are only used for inspecting how a model reaches\n",
       "its answer. One could potentially use the generated rationales to further check why the model makes\n",
       "certain mistakes or whether the model contains any biases when performing a certain task. For\n",
       "language model in real-world use, further work is needed to better ground models’ predictions and\n",
       "improve model’s factuality and safety, to ensure the models do not cause harms to users.\n",
       "</span></div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{~gen &#x27;关键信息&#x27; temperature=0.95 top_p=0.8 max_new_tokens=1024}}'>This paper proposes a new decoding strategy called self-consistency for language models, which significantly improves language models’ reasoning performance across a variety of tasks, including arithmetic and commonsense reasoning. The key innovation of self-consistency is to leverage the intuition that complex reasoning tasks typically admit multiple reasoning paths that reach a correct answer, and to use this insight to improve language models’ reasoning by a significant margin.\n",
       "\n",
       "To achieve this, the authors first prompt a language model with a set of manually written chain-of-thought exemplars, and then use a &quot;sample-and-marginalize&quot; decoding procedure to aggregate the answers by marginalizing out the sampled reasoning paths to find the most consistent answer in the final answer set. The self-consistency method can be applied to a wide range of reasoning tasks without any additional supervision or fine-tuning, while still substantially improving the performance of the chain-of-thought prompting approach proposed in previous work.\n",
       "\n",
       "Self-consistency is a simple yet effective method that incurs less computation cost than other approaches for improving language models’ reasoning performance. As the authors demonstrate in their experiments, self-consistency can achieve state-of-the-art levels of performance across a variety of tasks, including tasks that are difficult for language models to solve, such as the example in Table 4, where the model generates incorrect or nonsensical reasoning paths.\n",
       "\n",
       "Future work could focus on using self-consistency to generate better supervised data to fine-tune language models, as well as on grounding models’ rationale generations to improve their accuracy and reliability.</span></div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>根据这些关键信息，这段文章的主要贡献和改进点是什么？你需要用浅显易懂的语言总结知识，并指出其中的关键点。谢谢</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{gen &#x27;总结&#x27; temperature=0.95 top_p=0.8}}'>这段文章提出了一种新的方法，称为自校正（self-correcting）策略，旨在提高语言模型的推理能力。通过分析语言模型在推理过程中可能犯的错误，作者提出了一种简单而有效的策略：当模型在回答问题时，首先它会生成一系列短句子作为推理过程的提示，然后将这些句子中的每一句都进行“自我校正”，即根据一定的规则和上下文，选择最合理的句子来回答问题。\n",
       "\n",
       "这种方法的主要改进点在于：\n",
       "\n",
       "1. 引入了自校正机制，帮助语言模型在回答问题时更准确地推断答案。\n",
       "2. 是一种简单而有效的策略，不需要大量的训练数据或者复杂的算法，可以快速地应用于各种语言模型。\n",
       "3. 通过自我校正机制，语言模型可以更好地理解问题，从而生成更合理的答案。\n",
       "\n",
       "总之，自校正策略是一种有效的工具，可以帮助语言模型更好地进行推理，提高语言模型的性能。</span></div></div></pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"c6ee5951-9ab4-4a29-b394-f48b8220d498\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "program = guidance(\"\"\"{{#system~}}\n",
    "你是个拥有丰富经验的教授，负责给学生（用户）解答学术中的问题\n",
    "注意，先提取每段话的关键信息并输出出来，每个关键信息至少列举原文中的3个论据以支撑该观点。注意不要遗漏其中的任何观点\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "教授，请帮助我总结这个段落，列举其中的主要贡献\n",
    "\n",
    "标题：{{title}}\n",
    "正文：{{context}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{~gen '关键信息' temperature=0.95 top_p=0.8 max_new_tokens=1024}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{! 再根据之前分析的关键信息，进行归纳总结 }}\n",
    "\n",
    "{{#user~}}\n",
    "根据这些关键信息，这段文章的主要贡献和改进点是什么？你需要用浅显易懂的语言总结知识，并指出其中的关键点。谢谢\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen '总结' temperature=0.95 top_p=0.8}}\n",
    "{{~/assistant}}\"\"\", llm=llm)\n",
    "\n",
    "executed_program = program(title=title, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506965a5-2b44-4694-bf2c-2a96c78e83ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
