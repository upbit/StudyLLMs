{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "716650e3-2859-4e7e-afc5-578ab5bf723b",
   "metadata": {},
   "source": [
    "# 本地API测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b72b242-534c-4d5c-9082-065a9be610a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/models/codellama-7b-instruct.Q5_K_M.gguf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://127.0.0.1:5000/v1/models')\n",
    "models = [\n",
    "    model['id']\n",
    "    for model in r.json()['data']\n",
    "]\n",
    "\n",
    "model_name = models[0] # select first\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae710488-7b29-4460-bfbc-02d7969deeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是Assistant。我能够回答\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_base = \"http://127.0.0.1:5000/v1\"\n",
    "\n",
    "# create a chat completion\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=model_name,\n",
    "  messages=[{\"role\": \"user\", \"content\": \"你好，请自我介绍并说明你能给我提供的帮助\"}]\n",
    ")\n",
    "# print the completion\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "366cf538-0b27-4551-9db9-be6b9aed7d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您需要了解的是 `asyncio.wait()`\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "      {\"role\": \"user\", \"content\": \"\"\"您是个拥有丰富经验的AI助教，帮助或指引学生解决学习中遇到的问题。\n",
    "约束：对于已知问题的解答，需要通过 `从xxx的信息可以得出...` 说明其出处，其中xxx是实际存在的文章段落；对于未知或不确定的知识，请说明你不具备这部分知识，并提供一些相关主题的参考建议。\n",
    "我的第一个问题，请帮忙解释下下面代码：asyncio.wait()\"\"\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64221ba-7a46-4003-b10e-939c37075d85",
   "metadata": {},
   "source": [
    "# zhipuai bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55428519-18b6-4d8b-8267-fd9008e96674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import zhipuai\n",
    "from typing import AsyncGenerator, Tuple, Union\n",
    "from zhipuai.model_api.api import ModelAPI, InvokeType\n",
    "from zhipuai.utils import jwt_token\n",
    "\n",
    "zhipuai.api_key = \"65ee3a27e9a80f04abad9af2a99de2e8.sinGhMdqrHOblWRZ\"\n",
    "\n",
    "class DictObject(dict):\n",
    "    \"A dict base object like OpenAIObject\"\n",
    "    # https://github.com/openai/openai-python/blob/1be14ee34a0f8e42d3f9aa5451aa4cb161f1781f/openai/openai_object.py#L11\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        id=None,\n",
    "        **params,\n",
    "    ):\n",
    "        super(DictObject, self).__init__()\n",
    "\n",
    "        self._params = params\n",
    "\n",
    "        if id:\n",
    "            self[\"id\"] = id\n",
    "\n",
    "    def __setattr__(self, k, v):\n",
    "        if k[0] == \"_\" or k in self.__dict__:\n",
    "            return super(DictObject, self).__setattr__(k, v)\n",
    "\n",
    "        self[k] = v\n",
    "        return None\n",
    "\n",
    "    def __getattr__(self, k):\n",
    "        if k[0] == \"_\":\n",
    "            raise AttributeError(k)\n",
    "        try:\n",
    "            return self[k]\n",
    "        except KeyError as err:\n",
    "            raise AttributeError(*err.args)\n",
    "\n",
    "    def __delattr__(self, k):\n",
    "        if k[0] == \"_\" or k in self.__dict__:\n",
    "            return super(DictObject, self).__delattr__(k)\n",
    "        else:\n",
    "            del self[k]\n",
    "\n",
    "    def __setitem__(self, k, v):\n",
    "        if v == \"\":\n",
    "            raise ValueError(\n",
    "                \"You cannot set %s to an empty string. \"\n",
    "                \"We interpret empty strings as None in requests.\"\n",
    "                \"You may set %s.%s = None to delete the property\" % (k, str(self), k)\n",
    "            )\n",
    "        super(DictObject, self).__setitem__(k, v)\n",
    "\n",
    "    def __delitem__(self, k):\n",
    "        raise NotImplementedError(\"del is not supported\")\n",
    "\n",
    "    def __str__(self):\n",
    "        obj = self.to_dict_recursive()\n",
    "        return json.dumps(obj, indent=2)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return dict(self)\n",
    "\n",
    "    def to_dict_recursive(self):\n",
    "        d = dict(self)\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, DictObject):\n",
    "                d[k] = v.to_dict_recursive()\n",
    "            elif isinstance(v, list):\n",
    "                d[k] = [\n",
    "                    e.to_dict_recursive() if isinstance(e, DictObject) else e\n",
    "                    for e in v\n",
    "                ]\n",
    "        return d\n",
    "\n",
    "    @classmethod\n",
    "    def construct_from(cls, values):\n",
    "        instance = cls()\n",
    "        instance.refresh_from(values)\n",
    "        return instance\n",
    "\n",
    "    def refresh_from(self, values):\n",
    "        # Wipe old state before setting new.\n",
    "        self.clear()\n",
    "        for k, v in values.items():\n",
    "            super(DictObject, self).__setitem__(\n",
    "                k, convert_to_dict_object(v)\n",
    "            )\n",
    "\n",
    "        self._previous = values\n",
    "\n",
    "    # This class overrides __setitem__ to throw exceptions on inputs that it\n",
    "    # doesn't like. This can cause problems when we try to copy an object\n",
    "    # wholesale because some data that's returned from the API may not be valid\n",
    "    # if it was set to be set manually. Here we override the class' copy\n",
    "    # arguments so that we can bypass these possible exceptions on __setitem__.\n",
    "    def __copy__(self):\n",
    "        copied = DictObject()\n",
    "\n",
    "        copied._params = self._params\n",
    "\n",
    "        for k, v in self.items():\n",
    "            # Call parent's __setitem__ to avoid checks that we've added in the\n",
    "            # overridden version that can throw exceptions.\n",
    "            super(DictObject, copied).__setitem__(k, v)\n",
    "\n",
    "        return copied\n",
    "\n",
    "    # This class overrides __setitem__ to throw exceptions on inputs that it\n",
    "    # doesn't like. This can cause problems when we try to copy an object\n",
    "    # wholesale because some data that's returned from the API may not be valid\n",
    "    # if it was set to be set manually. Here we override the class' copy\n",
    "    # arguments so that we can bypass these possible exceptions on __setitem__.\n",
    "    def __deepcopy__(self, memo):\n",
    "        copied = self.__copy__()\n",
    "        memo[id(self)] = copied\n",
    "\n",
    "        for k, v in self.items():\n",
    "            # Call parent's __setitem__ to avoid checks that we've added in the\n",
    "            # overridden version that can throw exceptions.\n",
    "            super(OpenAIObject, copied).__setitem__(k, deepcopy(v, memo))\n",
    "\n",
    "        return copied\n",
    "\n",
    "def convert_to_dict_object(data):\n",
    "    if isinstance(data, list):\n",
    "        return [\n",
    "            convert_to_dict_object(v)\n",
    "            for v in data\n",
    "        ]\n",
    "    elif isinstance(data, dict) and not isinstance(data, DictObject):\n",
    "        return DictObject().construct_from(data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "class ZPChatCompletion(ModelAPI):\n",
    "    # OBJECT_NAME = \"chat.completions\"\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls,\n",
    "        api_key=None,\n",
    "        api_base=None,\n",
    "        api_type=None,\n",
    "        request_id=None,\n",
    "        api_version=None,\n",
    "        organization=None,\n",
    "        **params,\n",
    "     ):\n",
    "        \"\"\"\n",
    "        Creates a new chat completion for the provided messages and parameters.\n",
    "        \"\"\"\n",
    "        # if api_key: rewrite api_key\n",
    "        zhipu_params = cls.parse_to_zhipu_request(params)\n",
    "        response = super().invoke(**zhipu_params)\n",
    "        return cls.convert_to_openai_response(response)\n",
    "\n",
    "    @classmethod\n",
    "    async def acreate(cls,\n",
    "        api_key=None,\n",
    "        api_base=None,\n",
    "        api_type=None,\n",
    "        request_id=None,\n",
    "        api_version=None,\n",
    "        organization=None,\n",
    "        **params,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a new chat completion for the provided messages and parameters.\n",
    "        \"\"\"\n",
    "        zhipu_params = cls.parse_to_zhipu_request(params)\n",
    "        url = ModelAPI._build_api_url(zhipu_params, InvokeType.SYNC)\n",
    "        response = await APIRequestor().arequest(\n",
    "            \"post\",\n",
    "            url,\n",
    "            params=json.dumps(zhipu_params),\n",
    "            headers={\n",
    "                \"Authorization\": jwt_token.generate_token(zhipuai.api_key),\n",
    "                \"Accept\": \"application/json\",\n",
    "                \"Content-Type\": \"application/json; charset=UTF-8\",\n",
    "            },\n",
    "        )\n",
    "        return cls.convert_to_openai_response(await response.json())\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_to_zhipu_request(cls, params):\n",
    "        if 'messages' not in params:\n",
    "            raise Exception(\"messages param not found\")\n",
    "        messages = params.pop('messages')\n",
    "        zhipu_messages = []\n",
    "        for msg in messages:\n",
    "            # {\"role\": \"system|user|assistant\", \"content\": \"text\"}\n",
    "            if msg['role'] == 'system':  # not supported by ZhiPuAI, translate to user prompt\n",
    "                zhipu_messages.append({\"role\": 'user', \"content\": msg['content']})\n",
    "            else:\n",
    "                zhipu_messages.append(msg)\n",
    "        params['prompt'] = zhipu_messages\n",
    "        return params  \n",
    "\n",
    "    @classmethod\n",
    "    def convert_to_openai_response(cls, resp):\n",
    "        data = resp.pop('data')\n",
    "        request_id = data.pop('request_id')\n",
    "\n",
    "        choices = data.pop('choices')\n",
    "        openai_choices = []\n",
    "        if len(choices) > 0:\n",
    "            for i, msg in enumerate(choices):\n",
    "                openai_choices.append(convert_to_dict_object({\n",
    "                    \"message\": msg,\n",
    "                    \"finish_reason\": \"\",\n",
    "                    \"index\": i,\n",
    "                }))\n",
    "            openai_choices[-1]['finish_reason'] = \"stop\"\n",
    "\n",
    "        resp_object = convert_to_dict_object(resp)\n",
    "        resp_object.request_id = request_id\n",
    "        resp_object.choices = openai_choices\n",
    "        return resp_object\n",
    "\n",
    "class APIRequestor:\n",
    "\n",
    "    async def arequest(\n",
    "        self,\n",
    "        method,\n",
    "        url,\n",
    "        params=None,\n",
    "        headers=None,\n",
    "        proxy=None,\n",
    "        timeout=None,\n",
    "        # stream: bool = False,\n",
    "    ) -> Tuple[Union[DictObject, AsyncGenerator[DictObject, None]], bool, str]:\n",
    "        request_kwargs = {\n",
    "            \"method\": method,\n",
    "            \"url\": url,\n",
    "            \"headers\": headers,\n",
    "            \"data\": params,\n",
    "            \"proxy\": proxy,\n",
    "            \"timeout\": timeout,\n",
    "        }\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            try:\n",
    "                result = await session.request(**request_kwargs)\n",
    "                # Don't read the whole stream for debug logging unless necessary.\n",
    "                return result\n",
    "            except (aiohttp.ServerTimeoutError, asyncio.TimeoutError) as e:\n",
    "                raise error.Timeout(\"Request timed out\") from e\n",
    "            except aiohttp.ClientError as e:\n",
    "                raise error.APIConnectionError(\"Error communicating with ZhiPuAI\") from e\n",
    "\n",
    "import openai\n",
    "import importhook\n",
    "\n",
    "# Setup hook to be called any time the `openai` module is imported and loaded into module cache\n",
    "@importhook.on_import('openai')\n",
    "def on_openai_import(socket):\n",
    "    new_openai = importhook.copy_module(openai)\n",
    "    setattr(new_openai, 'ChatCompletion', ZPChatCompletion)\n",
    "    return new_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc99b1d-8061-43cc-b8e7-18a031e38d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" 你好！我是一名人工智能助手，我的主要任务是帮助你解决问题和提供有用的信息。我接受了大量的训练，可以回答各种领域的问题，包括但不限于科学、技术、历史、地理、文化、娱乐等。如果你有任何疑问或者需要帮助，请随时告诉我，我会尽力为你提供解答。\"\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"chatglm_pro\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"你好，请自我介绍并说明你能给我提供的帮助\"}]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0a7ee5-0817-4329-858e-f34b19f34898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" 你好！我是 ChatGLM，是清华大学 KEG 实验室和智谱 AI 公司共同训练的语言模型。我的目标是通过回答用户提出的问题来帮助他们解决问题。我可以回答各种问题，例如科学、数学、历史、文化、技术等等。如果有任何问题需要帮助，请随时问我。\"\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "acompletion = await openai.ChatCompletion.acreate(\n",
    "  model=\"chatglm_pro\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"你好，请自我介绍并说明你能给我提供的帮助\"}]\n",
    ")\n",
    "print(acompletion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9937f39a-f59c-49fb-9781-1080ac8a0fb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mloguru\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIChatCompletion\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\Lib\\site-packages\\importhook\\loader.py:85\u001b[0m, in \u001b[0;36mHookLoader.exec_module\u001b[1;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexec_module\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     module \u001b[38;5;241m=\u001b[39m mod\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\Lib\\site-packages\\semantic_kernel\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Microsoft. All rights reserved.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_skills, memory\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Kernel\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morchestration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_variables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextVariables\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\Lib\\site-packages\\importhook\\loader.py:85\u001b[0m, in \u001b[0;36mHookLoader.exec_module\u001b[1;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexec_module\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     module \u001b[38;5;241m=\u001b[39m mod\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\Lib\\site-packages\\semantic_kernel\\memory\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Microsoft. All rights reserved.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvolatile_memory_store\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VolatileMemoryStore\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVolatileMemoryStore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\Lib\\site-packages\\importhook\\loader.py:85\u001b[0m, in \u001b[0;36mHookLoader.exec_module\u001b[1;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexec_module\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     module \u001b[38;5;241m=\u001b[39m mod\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\Lib\\site-packages\\semantic_kernel\\memory\\volatile_memory_store.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory_record\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MemoryRecord\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory_store_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MemoryStoreBase\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnull_logger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NullLogger\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mVolatileMemoryStore\u001b[39;00m(MemoryStoreBase):\n\u001b[0;32m     15\u001b[0m     _store: Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, MemoryRecord]]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:674\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:634\u001b[0m, in \u001b[0;36m_load_backward_compatible\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\Lib\\site-packages\\importhook\\loader.py:49\u001b[0m, in \u001b[0;36mHookLoader.__getattribute__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(loader, name)\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import semantic_kernel as sk\n",
    "from loguru import logger\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.connectors.ai.chat_request_settings import ChatRequestSettings\n",
    "\n",
    "settings = ChatRequestSettings(temperature=0.95, top_p=0.7)\n",
    "chat = OpenAIChatCompletion(\"chatglm_pro\", \"noop\", log=logger)\n",
    "resp = await chat.complete_chat_async(messages=[(\"user\", \"你好，请自我介绍并说明你能给我提供的帮助\")],\n",
    "                         request_settings=settings, logger=logger)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1342d-ca91-40d5-9abe-46d311fc6a4d",
   "metadata": {},
   "source": [
    "# Semantic Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71c4de19-3a89-4eff-a449-eb6b0b27148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "\n",
    "# import openai\n",
    "# openai.api_base = \"http://127.0.0.1:5001/v1\"\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "36c967f8-924a-4e07-8ef5-88787c82810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "kernel.add_text_completion_service(\"openai\", OpenAIChatCompletion(\"chatglm_pro\", \"noop\", \"\"))\n",
    "\n",
    "sk_prompt = \"\"\"\n",
    "您是一位经验丰富的AI商业分析助手。您的主要目标是分辨[问题]涉及的内容，并从[文章]抽取该内容相关的4个客观观点，用于下一步在线搜索的Google查询语句。约束：请考虑该查询是否可以检索到包含业务价值、市场趋势和战略分析相关的问题，以便后续生成全面、富有洞察力、公正并且系统化结构的商业分析报告。\n",
    "\n",
    "您必须以json代码块的格式输出结果，格式参考:\n",
    "`[\"query 1\", \"query 2\", \"query 3\", \"query 4\"]`\n",
    "\n",
    "注意：查询query必须是中文\n",
    "\n",
    "{{$input}}\n",
    "\"\"\"\n",
    "summary_function = kernel.create_semantic_function(prompt_template = sk_prompt,\n",
    "                                                    description=\"观点抽取\",\n",
    "                                                    max_tokens=200,\n",
    "                                                    temperature=0.1,\n",
    "                                                    top_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "43771ed5-9fcd-4841-97ea-71f87bbc6b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ✨ Error: (<ErrorCodes.ServiceError: 6>, 'OpenAI service failed to complete the chat', APIConnectionError(message='Error communicating with OpenAI', http_status=None, request_id=None))"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sk_input = \"\"\"\n",
    "主题：AI 与 Web3 的布局机会：投资人眼中的前景和机遇\n",
    "\n",
    "文章：\n",
    "上次写完BTC生态的东西，本来该补上NFT，NFTFI的那篇，奈何NFT最近真的不是一般凉，不光二级市场凉，一级市场我好像有俩月都没聊到NFT或是NFTFI相关的项目了，倒是AI项目真的如井喷一般的涌现，所以NFT那篇继续拖着吧，把AI+Web3的结合趋势这篇先提上来。\n",
    "\n",
    "一 . 先说说AI自身\n",
    "AI这行业其实本来都要凉凉了，大家知道Near的创始人一龙对吧，这家伙其实以前是做AI的，他是TensorFlow（最流行的机器学习框架）的主要代码贡献者。大家推测他是AI（大模型之前的机器学习）那边看不到啥希望了所以跑来做Web3的。\n",
    "\n",
    "结果终于去年年底业界迎来了ChatGpt3.5，一下子这行业又活了，因为这次真的可以算质变了，而不是之前那几波的炒作和量变。这不隔了几个月AI创业的浪潮也传递到了我们Web3。硅谷Web2那边则是卷的不行，各种资本Fomo，各种同质化方案开始拼价格战，各种大厂大模型PK……\n",
    "\n",
    "但是要注意到的是AI经历了半年多的爆发期之后也进入了一个相对瓶颈期，比如Google对与AI的搜索热度断崖式下跌，Chatgpt用户增速大幅放缓，AI Output带有一定的随机性限制了许多落地场景……总而言之，我们离传说中的“AGI - 通用人工智能”还有非常非常远的距离。\n",
    "\n",
    "目前硅谷创投圈对与AI下一步发展有这么几个判断 ：\n",
    "\n",
    "1. 没有垂类模型，只有大模型+垂类应用（一会儿说Web3+AI的时候我们会再提到）；\n",
    "\n",
    "2. 边缘设备比如手机端的数据可能会是个壁垒，基于边缘设备的AI可能也是个机会；\n",
    "\n",
    "3. Context的长度未来可能引发质变（现在用向量数据库作为AI记忆体，但上下文长度还是不够）。\n",
    "\n",
    "二. Web3+AI\n",
    "AI和Web3其实是完全不同的两个领域，AI需要集中的算力+海量数据做训练，非常中心化的东西，Web3则是主打一个去中心化，所以其实不是那么好结合，单奈何叙事上AI改变生产力，区块链改变生产关系这个论点太过深入人心，所以总会有人前仆后继的去寻找那个结合点，近俩月得聊了不下10个AI项目。\n",
    "\n",
    "在说新的结合赛道之前先说说老的AI+Web3项目，基本都是平台型，以FET和AGIX为代表。怎么说呢，我国内专业做AI的朋友是这么跟我说的 - “以前这些做AI的现在基本都没啥用了，无论Web2还是Web3,很多都是包袱而不是经验。方向和未来就是像OpenAI的这种基于Transformer的大模型，大模型拯救了AI”，你自己品。\n",
    "\n",
    "所以通用平台型不是他所看好的Web3+AI的模式，我聊的这10多个项目也确实没有这方面，目前看到的基本是如下几个赛道：\n",
    "\n",
    "1. Bot/Agent/Assistant 模型资产化\n",
    "2. 算力平台\n",
    "3. 数据平台\n",
    "4. 生成式AI\n",
    "5. Defi交易/审计/风控\n",
    "6. ZKML\n",
    "\n",
    "今天主要详细说下1，也就是Bot/Agent/Assitant的资产化这个赛道，这也是聊的最多，同质化最为严重的一个赛道 简单来说，这些项目多是拿OpenAI为底层，配合其他的一些开源/自研的技术手段，比如TTS（Text to Speech）之类，加上特定的数据，FineTune出来一些“某一领域比ChatGPT”更好的机器人。\n",
    "\n",
    "比如你可以训练出一个教你英语的美女老师，你可以选择她是美国口音还是伦敦腔，她的性格和聊天的方式也可以调整，这样相对于ChatGPT比较机械和官方的回答来讲，你的交互体验会更好一些。圈内前段有个虚拟男友的DAPP，Web3女性向游戏，叫HIM，可以算是这种类型的代表了。\n",
    "\n",
    "从这个思路出发，你理论上可以有许多个Bot/Agent为你服务，比如你想要做水煮鱼，可能会有专门针对这个领域Fine Tune的Cooking Bot来教你，给的答案相对ChatGPT更加专业，你想出门旅行，同样有旅行小助手Bot给你提供各种出游建议和规划，或是你是项目方，弄一个Discord的客服机器人，帮你回答社区问题。\n",
    "\n",
    "除了做这种“基于GPT的垂类应用型”Bot，还有基于此的衍生项目，比如Bot算“模型资产化”，有点NFT“小图片资产化”的意味，那现在AI里面流行的Prompt是不是也可以资产化，像是MidJourney不同的Prompt可以生成不同的图片，训练Bot时不同的Prompt也会有不同的效果，所以Promopt自身也具备价值，也可以资产化。\n",
    "\n",
    "还有像是基于此类Bot进行门户索引，搜索的项目，哪天我们有了成千上万的Bot，怎么找到最合适你的Bot？可能届时就需要一个Web2世界类似Hao123这样的门户，或是Google这样的搜索引擎来帮你“定位”。\n",
    "\"\"\";\n",
    "\n",
    "summary_result = await kernel.run_async(summary_function, input_str=sk_input)\n",
    "\n",
    "display(Markdown(\"### ✨ \" + str(summary_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225045d6-d82f-485b-b140-343636706298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
